{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTBarDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 index: [float, float]=[0.0, 1.0]):\n",
    "        \n",
    "        data=np.load(\"../sample_data/parton_level.npy\")\n",
    "        num = data.shape[0]\n",
    "        source =[]\n",
    "        for k in data:\n",
    "            particles =k[0:16].reshape((4,4))\n",
    "            source.append(particles)\n",
    "        self.source = np.array(source)        \n",
    "        \n",
    "        index = (int(round(index[0] * num)), int(round(index[1] * num)))\n",
    "        indices = np.arange(num)[index[0]:index[1]]\n",
    "        self.num_samples = indices.shape[0]\n",
    "        \n",
    "        self.source = self.source[indices]\n",
    "        self.indices = indices\n",
    "        \n",
    "        source = self.source\n",
    "        source_inv = source[:,[0,3,2,1]]\n",
    "        \n",
    "        label = np.ones(self.num_samples)\n",
    "        label_inv = np.zeros(self.num_samples)\n",
    "        \n",
    "        source = np.concatenate((source, source_inv))\n",
    "        label = np.concatenate((label, label_inv))\n",
    "        \n",
    "        idx = np.arange(self.num_samples*2)\n",
    "        np.random.shuffle(idx)\n",
    "        source, label =  source[idx], label[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.source = torch.from_numpy(source).float()\n",
    "        self.targets = torch.from_numpy(label).float() \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = np.copy(self.source[idx])\n",
    "        y = np.copy(self.targets[idx])\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "class TTBarDataset_detector(Dataset):\n",
    "    def __init__(self,\n",
    "                 index: [float, float]=[0.0, 1.0]):\n",
    "        data=np.load(\"../sample_data/detector_level.npy\")\n",
    "        num = data.shape[0]\n",
    "        source =[]\n",
    "        for k in data:\n",
    "            particles =k[0:16].reshape((4,4))\n",
    "            source.append(particles)\n",
    "        self.source = np.array(source)        \n",
    "        \n",
    "        index = (int(round(index[0] * num)), int(round(index[1] * num)))\n",
    "        indices = np.arange(num)[index[0]:index[1]]\n",
    "        self.num_samples = indices.shape[0]\n",
    "        \n",
    "        self.source = self.source[indices]\n",
    "        self.indices = indices\n",
    "        \n",
    "        source = self.source\n",
    "        source_inv = source[:,[0,3,2,1]]\n",
    "        \n",
    "        label = np.ones(self.num_samples)\n",
    "        label_inv = np.zeros(self.num_samples)\n",
    "        \n",
    "        source = np.concatenate((source, source_inv))\n",
    "        label = np.concatenate((label, label_inv))\n",
    "        \n",
    "        idx = np.arange(self.num_samples*2)\n",
    "        np.random.shuffle(idx)\n",
    "        source, label =  source[idx], label[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.source = torch.from_numpy(source).float()\n",
    "        self.targets = torch.from_numpy(label).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = np.copy(self.source[idx])\n",
    "        y = np.copy(self.targets[idx])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_stack(input_dim, output_dim):\n",
    "    layers = [nn.Linear(input_dim, output_dim)]\n",
    "\n",
    "    \n",
    "    layers.append(nn.PReLU(output_dim))\n",
    "    \n",
    "\n",
    "    \n",
    "    layers.append(nn.BatchNorm1d(output_dim))\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "\n",
    "    return layers\n",
    "\n",
    "\n",
    "def create_linear_layers(num_layers: int, hidden_dim: int):\n",
    "    layers = []\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        layers.extend(create_linear_stack(hidden_dim, hidden_dim))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, transformer_options: Tuple[int, int, int, float, str]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = self.create_embedding_layers(input_dim)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer(*transformer_options), num_encoder_layers)\n",
    "        \n",
    "\n",
    "    def create_embedding_layers(self, input_dim):\n",
    "        current_embedding_dim = initial_embedding_dim\n",
    "        embedding_layers = create_linear_stack(input_dim, current_embedding_dim)\n",
    "\n",
    "        for i in range(num_embedding_layers):\n",
    "            next_embedding_dim = 2 * current_embedding_dim\n",
    "            if next_embedding_dim >= hidden_dim:\n",
    "                break\n",
    "\n",
    "            embedding_layers.extend(create_linear_stack(current_embedding_dim, next_embedding_dim))\n",
    "            current_embedding_dim = next_embedding_dim\n",
    "\n",
    "        embedding_layers.extend(create_linear_stack(current_embedding_dim, hidden_dim))\n",
    "\n",
    "        return nn.Sequential(*embedding_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        batch_size, max_particles, input_dim = x.shape\n",
    "\n",
    "        hidden = self.embedding(x.view(-1, input_dim))\n",
    "        hidden = hidden.view(batch_size, max_particles, hidden_dim)\n",
    "\n",
    "        hidden = hidden.transpose(0, 1)\n",
    "\n",
    "        hidden = self.encoder(hidden) \n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 512\n",
    "num_dataloader_workers = 16\n",
    "hidden_dim = 64\n",
    "num_attention_heads = 4\n",
    "dropout = 0.1\n",
    "transformer_activation = 'relu'\n",
    "initial_embedding_dim = 8\n",
    "num_encoder_layers = 3\n",
    "learning_rate = 0.001\n",
    "l2_penalty = 9e-05\n",
    "num_embedding_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TTBarDataset(index=[0.0,0.8])\n",
    "validation_dataset = TTBarDataset(index=[0.8,0.9])\n",
    "testing_dataset = TTBarDataset(index=[0.9,1.0])\n",
    "# training_dataset = TTBarDataset_detector(index=[0.0,0.8])\n",
    "# validation_dataset = TTBarDataset_detector(index=[0.8,0.9])\n",
    "# testing_dataset = TTBarDataset_detector(index=[0.9,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader() -> DataLoader:\n",
    "    return DataLoader(training_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=True,\n",
    "                        num_workers=num_dataloader_workers,\n",
    "                        pin_memory=True)\n",
    "def val_dataloader() -> DataLoader:\n",
    "    return DataLoader(validation_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=True,\n",
    "                          num_workers=num_dataloader_workers,\n",
    "                          pin_memory=True)\n",
    "\n",
    "def testing_dataloader() -> DataLoader:\n",
    "    return DataLoader(testing_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=True,\n",
    "                          num_workers=num_dataloader_workers,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ttbarNetwork(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_particles = 4\n",
    "\n",
    "        transformer_options = (self.hidden_dim,\n",
    "                               num_attention_heads,\n",
    "                               self.hidden_dim,\n",
    "                               dropout,\n",
    "                               transformer_activation)\n",
    "\n",
    "        self.encoder = ParticleEncoder( 4, transformer_options)\n",
    "\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.embedding2 = nn.Sequential(nn.Linear(hidden_dim*4,hidden_dim))\n",
    "        self.embedding3 = nn.Sequential(nn.Linear(hidden_dim,1))\n",
    "        self.hid_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        # Extract features from data using transformer\n",
    "        input_dim = self.hid_dim \n",
    "        q = self.encoder(x)\n",
    "        a,batch_size, b = q.shape\n",
    "        x = q.transpose(0, 1)\n",
    "        x = x.reshape(-1, a*b)\n",
    "        x = self.embedding2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.embedding3(x)\n",
    "        \n",
    "\n",
    "        output = self.activation(x)\n",
    "\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, targets = batch\n",
    "\n",
    "        predictions = self.forward(x)\n",
    "        \n",
    "\n",
    "        comb_loss = self.loss(predictions, targets)\n",
    "        \n",
    "        loss = torch.mean(comb_loss)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Training loss has diverged.\")\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(predictions: Tensor, targets: Tensor) -> Tensor:\n",
    "        \"\"\" Compute single top and eventy accuracy for a batch. \"\"\"\n",
    "        l_predictions = predictions.clone()\n",
    "        \n",
    "        l_predictions = l_predictions.round()\n",
    "        \n",
    "\n",
    "        l_targets = targets.clone()\n",
    "\n",
    "        accuracy = l_targets == l_predictions\n",
    "\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        optimizer = torch.optim.Adam\n",
    "\n",
    "        return optimizer(self.parameters(), lr=learning_rate, weight_decay=l2_penalty)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets = batch\n",
    "        predictions = self.forward(x)\n",
    "        all_loss = self.loss(predictions, targets)\n",
    "        \n",
    "        val_loss = torch.mean(all_loss)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        accuracy = self.accuracy(predictions, targets)\n",
    "\n",
    "\n",
    "        accuracy = accuracy.float().mean()\n",
    "\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        average_accuracy = torch.mean(torch.stack([x['accuracy'] for x in outputs]))\n",
    "\n",
    "        print(average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ttbarNetwork()\n",
    "trainer = pl.Trainer(max_epochs=300, gpus=1, precision= 32,callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloader(), val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ttbarNetwork.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.to(device='cuda:0')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device='cuda:0')\n",
    "            y = y.to(device='cuda:0')\n",
    "            \n",
    "            scores = model(x)\n",
    "            predictions = scores.round()\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "    return float(num_correct)/float(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_accuracy(testing_dataloader(), model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
